{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PGDMLAI - IIIT Bangalore (Upgrad) ####\n",
    "#### Gesture Recognition Case Study ####\n",
    "Team Members:\n",
    "1. Abhishek Rajan\n",
    "2. Aiswarya Ramachandran\n",
    "3. Anugraha Sinha\n",
    "4. Vikash Sinha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gesture Recognition (Using Conv2D + GRU technique)\n",
    "In this case, we will be using a CNN2D + GRU (Gated Recurrent Unit) Neural Network for evaluating geatures in videos. \n",
    "The data provided as following features\n",
    "1. 30 Video frames in each video\n",
    "2. Each frame - 3 Channel (RGB)\n",
    "3. 5 kinds of gestures (Up, Down, Right, Left, Stop)\n",
    "\n",
    "The objective is to be fullfilled for a hypothetical Smart TV company which is trying to incorporate geature recognition in its product where in the gestures mean following things\n",
    "\n",
    "Gesture   |       Left       |    Right    |    Stop      |    Down     |     Up\n",
    "----------| -----------------|-------------|--------------|-------------|-----------\n",
    "Objectives| Previous Channel | Next Channel| Stop Playing | Volume Down | Volume Up\n",
    "Y Value (Integer) | 0 | 1 | 2 | 3 | 4 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Library imports #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from scipy.misc import imread, imresize\n",
    "import datetime\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### We set the random seed so that the results don't vary drastically.#####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(30)\n",
    "import random as rn\n",
    "rn.seed(30)\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "tf.set_random_seed(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### We set the train_doc and val_doc location #####\n",
    "*Note : Please set this location as per your environment.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_doc = np.random.permutation(open('../../../Project_data/train.csv').readlines())\n",
    "val_doc = np.random.permutation(open('../../../Project_data/val.csv').readlines())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Image Processor ####\n",
    "The objective of this function is:\n",
    "Resize the images as per *transform_size* given by user, or take it as (120,120)\n",
    "\n",
    "**Reason**\n",
    "\n",
    "In the training data we have 2 types of video, where video frame sizes are as *(360,360)* and *(160,120)* Therefore to keep things consistent, we build a image processor which perform cropping of the image.\n",
    "\n",
    "**Note**\n",
    "\n",
    "Cropping is done as a centered image, i.e., we take the center as the reference and crop the images from both sides. For (360,360) we can use skimage.transform.resize function. However, for *(160,120)* we will have to some manual processing to crop images by the center."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.transform import resize\n",
    "def image_processor(orig_image,transform_size=(120,120)):\n",
    "    # If we have an image of (360,360,3) then we use resize function.\n",
    "    # If we have an image of \n",
    "    new_image = orig_image\n",
    "    if orig_image.shape == (360,360,3):\n",
    "        new_image = resize(orig_image,transform_size)\n",
    "    else:\n",
    "        start_row = (orig_image.shape[0] - transform_size[0])//2\n",
    "        end_row = start_row + transform_size[0]\n",
    "        \n",
    "        start_col = (orig_image.shape[1] - transform_size[1])//2\n",
    "        end_col = start_col + transform_size[1]\n",
    "        new_image = orig_image[start_row:end_row,start_col:end_col,:]\n",
    "    #new_image = resize(orig_image,transform_size)\n",
    "    return(new_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sequence Generator ####\n",
    "This function provides a list of frame sequences we would like to use training\n",
    "\n",
    "Arguments\n",
    "\n",
    "a) choiceoflist\n",
    "\n",
    "\n",
    "choicelist | 0 | 1 | 2\n",
    "-----------|---|---|---\n",
    "Return list type | range(0,30,1) | range(0,30,2) | [0,1,2,3,4,5,6,9,12,15,18,21,24,25,26,27,28,29]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getframeselectionlist(choiceoflist=0):\n",
    "    if choiceoflist==0:\n",
    "        return [frame for frame in range(0,30,1)] # Returns 100% of frames, number of frames=30\n",
    "    elif choiceoflist==1:\n",
    "        return [frame for frame in range(0,30,2)] # returns 50% of frames, number of frames=15\n",
    "    elif choiceoflist==2:\n",
    "        \n",
    "        # For this we are taking first 5 frame+skip frame sequence+last 5 frame of the sequence\n",
    "        \n",
    "        frame_sequence=[]\n",
    "        \n",
    "        startframesequence=[0,1,2,3,4,5]\n",
    "        endframesequence=[25,26,27,28,29]\n",
    "        skip_sequence=3  #\n",
    "        middleframesequnce=[k for k in range(6,25,skip_sequence)]\n",
    "        \n",
    "        frame_sequence.extend(startframesequence)\n",
    "        frame_sequence.extend(middleframesequnce)\n",
    "        frame_sequence.extend(endframesequence)\n",
    "        \n",
    "        return frame_sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generator ###\n",
    "This is heart of complete training process. It pumps batched data to network during learning and prediction both. The function description is given below\n",
    "\n",
    "**Arguments**\n",
    "\n",
    "1. Source Path - Directory path to be considered for reading video/images frames\n",
    "2. folder_list - Lines from the train_doc we read above.\n",
    "3. batch_size - The batch_size we want to select.\n",
    "4. transform_size - The image transformation size we  (Default - (120,120)\n",
    "5. frame_selection_list - frame_list obtained from frame_generator (Default - range(30))\n",
    "6. process_input_func - To be provided in case CNN2D+RNN type modelling being done\n",
    "7. base_model - To be provided in case CNN2D+RNN type modelling being done.\n",
    "\n",
    "**Working**\n",
    "* Case when CNN3D modelling being done\n",
    "\n",
    "In this case, for each batch (according to batch size), we build \n",
    "\n",
    "1. **batch_data** = *(batch_size, number_of_frames,image_size_x,image_size_y,n_channels)*\n",
    "2. We normalize each channel (RGB) by dividing the pixel value with 255.\n",
    "\n",
    "\n",
    "* Case when CNN2D+RNN modelling being done (RNN can be any of SimpleRNN/LSTM/GRU)\n",
    "\n",
    "In this case, for each batch (according to batch size), we build\n",
    "\n",
    "1. **batch_data** = *(batch_size, number_of_frames,image_size_x,image_size_y,n_channels)*\n",
    "2. reshape batch data as **batch_data.reshape(batch_size * number_of_frames , image_size_x , image_size_y , n_channels)**.\n",
    "3. Above reshaped numpy array is sent to *process_input_func* of the pre-learned CNN2D function. This will produce modified image vector as per pre-learned CNN2D function (like VGG19/VGG16/etc.)\n",
    "4. After *process_input_func* we reshape again to *(batch_size, number_of_frames, outputs from CNN2D vector)*\n",
    "\n",
    "\n",
    "* Final Output\n",
    "\n",
    "The final output of the function has a tuple which has the batch_data (processed) and one-hot-encoded Y variable.\n",
    "One-hot-encoded numpy array will be of size (batch_size, 5) since we have 5 kind of gestures.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(source_path,\n",
    "              folder_list,\n",
    "              batch_size,\n",
    "              transform_size = (120,120),\n",
    "              frame_selection_list = range(30),\n",
    "              process_input_func=None,\n",
    "              base_model = None):\n",
    "    print( 'Source path = ', source_path, '; batch size =', batch_size)\n",
    "    img_idx = frame_selection_list\n",
    "    channels = 3\n",
    "    while True:\n",
    "        t = np.random.permutation(folder_list)\n",
    "        num_batches = int(len(t) / batch_size) if len(t) % batch_size == 0 else (len(t) // batch_size)\n",
    "        for batch in range(num_batches): # we iterate over the number of batches\n",
    "            batch_data = np.zeros((batch_size,len(frame_selection_list),transform_size[0],transform_size[1],channels)) # x is the number of images you use for each video, (y,z) is the final size of the input images and 3 is the number of channels RGB\n",
    "            batch_labels = np.zeros((batch_size,5)) # batch_labels is the one hot representation of the output\n",
    "            for folder in range(batch_size): # iterate over the batch_size\n",
    "                imgs = os.listdir(source_path+'/'+ t[folder + (batch*batch_size)].split(';')[0]) # read all the images in the folder\n",
    "                #print(t[folder + (batch*batch_size)].split(';')[0])\n",
    "                for idx,item in enumerate(img_idx): #  Iterate iver the frames/images of a folder to read them in\n",
    "                    image = imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
    "                    #crop the images and resize them. Note that the images are of 2 different shape \n",
    "                    #and the conv3D will throw error if the inputs in a batch have different shapes\n",
    "                    image = image_processor(image,transform_size)\n",
    "                    if base_model:\n",
    "                        # This is when we are using a pre-learned CNN2D network\n",
    "                        image = process_input_func(image)\n",
    "                        \n",
    "                    else:\n",
    "                        # This is when we are building a Conv3D network\n",
    "                        image = image/255\n",
    "\n",
    "                    batch_data[folder,idx,:,:,0] = image[:,:,0]\n",
    "                    batch_data[folder,idx,:,:,1] = image[:,:,1]\n",
    "                    batch_data[folder,idx,:,:,2] = image[:,:,2]\n",
    "                    \n",
    "                batch_labels[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n",
    "            if base_model:\n",
    "                s1 = batch_data.shape\n",
    "                n1 = base_model.predict(batch_data.reshape(s1[0]*s1[1],s1[2],s1[3],s1[4]))\n",
    "                s2 = n1.shape\n",
    "                batch_data = n1.reshape(s1[0],s1[1],s2[1]*s2[2]*s2[3])\n",
    "            yield batch_data, batch_labels #you yield the batch_data and the batch_labels, remember what does yield do\n",
    "        \n",
    "        # write the code for the remaining data points which are left after full batches\n",
    "        # remaining data #\n",
    "        if len(t) % batch_size != 0:                                      # Execute only if, we need to\n",
    "            batch_data = np.zeros((len(t) % batch_size,len(frame_selection_list),transform_size[0],transform_size[1],channels))     # fix the last batch size\n",
    "            batch_labels = np.zeros((len(t) % batch_size,5))              # Similarly, for labels\n",
    "            for v_idx,folder in enumerate(t[(num_batches*batch_size):]):\n",
    "                #print(folder.split(';')[0])\n",
    "                imgs = os.listdir(source_path+\"/\"+folder.split(';')[0])\n",
    "                for idx, item in enumerate(img_idx):\n",
    "                    image = imread(source_path+'/'+folder.split(';')[0]+\"/\"+imgs[item]).astype(np.float32)\n",
    "                    image = image_processor(image,transform_size)\n",
    "\n",
    "                    if base_model:\n",
    "                        # This is when we are using a pre-learned CNN2D network\n",
    "                        image = process_input_func(image)\n",
    "                    else:\n",
    "                        # This is when we are building a Conv3D network\n",
    "                        image = image/255\n",
    "\n",
    "                    batch_data[v_idx,idx,:,:,0] = image[:,:,0]\n",
    "                    batch_data[v_idx,idx,:,:,1] = image[:,:,1]\n",
    "                    batch_data[v_idx,idx,:,:,0] = image[:,:,2]\n",
    "                batch_labels[v_idx,int(folder.split(\";\")[2])] = 1\n",
    "            if base_model:\n",
    "                s1 = batch_data.shape\n",
    "                n1 = base_model.predict(batch_data.reshape(s1[0]*s1[1],s1[2],s1[3],s1[4]))\n",
    "                s2 = n1.shape\n",
    "                batch_data = n1.reshape(s1[0],s1[1],s2[1]*s2[2]*s2[3])\n",
    "            yield batch_data,batch_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recording *current_date-time* and fixing training data and validation data folder paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_dt_time = datetime.datetime.now()\n",
    "train_path = '../../../Project_data/train'\n",
    "val_path = '../../../Project_data/val'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "Setting up base information for model building process.\n",
    "\n",
    "*IMPORTANT*\n",
    "We are using VGG19 pre-trained model for CONV2D modelling.\n",
    "\n",
    "If this code is CONV3D, then even though we are building the *base_model* and *preprocess_input_func* variables, but we will not use them further in the code.\n",
    "\n",
    "If this code is CONV2D+RNN type modelling, then we will use *base_model* and *preprocess_input_func* variables further in the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_doc\n",
    "val_data = val_doc\n",
    "transform_size = (120,120)\n",
    "batch_size = 16\n",
    "num_epochs = 50\n",
    "frame_selection_list = getframeselectionlist(choiceoflist=0)\n",
    "num_train_sequences = len(train_data)\n",
    "num_val_sequences = len(val_data)\n",
    "model_name = \"model_init_conv2d_gru\"\n",
    "\n",
    "from keras.applications.vgg19 import VGG19 as base_conv2d_model\n",
    "from keras.applications.vgg19 import preprocess_input\n",
    "\n",
    "base_model = base_conv2d_model(weights=\"imagenet\",include_top=False)\n",
    "preprocess_input_func = preprocess_input\n",
    "\n",
    "os.environ.putenv(\"HDF5_USE_FILE_LOCKING\",\"FALSE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building Neural Network Model Architecture ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source path =  ../../../Project_data/train ; batch size = 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/fastai/lib/python3.6/site-packages/ipykernel_launcher.py:21: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n",
      "/opt/conda/envs/fastai/lib/python3.6/site-packages/skimage/transform/_warps.py:105: UserWarning: The default mode, 'constant', will be changed to 'reflect' in skimage 0.15.\n",
      "  warn(\"The default mode, 'constant', will be changed to 'reflect' in \"\n",
      "/opt/conda/envs/fastai/lib/python3.6/site-packages/skimage/transform/_warps.py:110: UserWarning: Anti-aliasing will be enabled by default in skimage 0.15 to avoid aliasing artifacts when down-sampling images.\n",
      "  warn(\"Anti-aliasing will be enabled by default in skimage 0.15 to \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input batch shape : (16, 30, 4608)\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, GRU, Flatten, TimeDistributed, Flatten, BatchNormalization, Activation, Dropout, Bidirectional, SimpleRNN, LSTM, GRU\n",
    "from keras.layers.convolutional import Conv3D, MaxPooling3D\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras import optimizers\n",
    "\n",
    "input_sample = next(generator(train_path,\n",
    "                              train_data, \n",
    "                              batch_size=batch_size,\n",
    "                              transform_size=transform_size,\n",
    "                              frame_selection_list=frame_selection_list,\n",
    "                              process_input_func=preprocess_input_func,\n",
    "                              base_model=base_model))\n",
    "print(\"Input batch shape : %s\" %(str(input_sample[0].shape)))\n",
    "model = Sequential()\n",
    "model.add(GRU(64,\n",
    "              return_sequences=True,\n",
    "              input_shape=(input_sample[0].shape[1],input_sample[0].shape[2])))\n",
    "model.add(GRU(64))\n",
    "\n",
    "model.add(Dense(input_sample[1].shape[1], activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting up Neural Network configurations ####\n",
    "\n",
    "Optimizers\n",
    "loss function and metrics to be monitored.\n",
    "\n",
    "Watch out for model summary for verifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_1 (GRU)                  (None, 30, 64)            897216    \n",
      "_________________________________________________________________\n",
      "gru_2 (GRU)                  (None, 64)                24768     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 5)                 325       \n",
      "=================================================================\n",
      "Total params: 922,309\n",
      "Trainable params: 922,309\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "optimizer = optimizers.SGD(lr=0.01)\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "print (model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building up generator objects ####\n",
    "\n",
    "train_generator for training data\n",
    "\n",
    "val_generator for validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = generator(train_path,\n",
    "                            train_data, \n",
    "                            batch_size=batch_size,\n",
    "                            transform_size=transform_size,\n",
    "                            frame_selection_list=frame_selection_list,\n",
    "                            process_input_func=preprocess_input_func,\n",
    "                            base_model=base_model)\n",
    "val_generator = generator(val_path,\n",
    "                          val_data, \n",
    "                          batch_size=batch_size,\n",
    "                          transform_size=transform_size,\n",
    "                          frame_selection_list=frame_selection_list,\n",
    "                          process_input_func=preprocess_input_func,\n",
    "                          base_model=base_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting up extra parameter for Neural Network ####\n",
    "\n",
    "* ModelCheckPoint\n",
    "\n",
    "We build modelcheckpoint to save models after every epoch, so that we can refer to models to best model later on for final testing.\n",
    "* ReduceLROnPlateau\n",
    "\n",
    "If the **val_loss** value stops improving after **patience** number of epochs, we reduce the learning rate so as to take smaller steps down the gradient route for raeaching global minimum and avoid getting stuch at local minimum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_init_conv2d_gru_2018-12-2801_12_09.769114/\n"
     ]
    }
   ],
   "source": [
    "model_name = model_name + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n",
    "\n",
    "if not os.path.exists(model_name):\n",
    "    os.mkdir(model_name)\n",
    "        \n",
    "filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
    "\n",
    "LR = ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n",
    "                       patience=5, min_lr=0.001,verbose=1)\n",
    "\n",
    "callbacks_list = [checkpoint, LR]\n",
    "#callbacks_list = [checkpoint]\n",
    "#callbacks_list = []\n",
    "print(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `steps_per_epoch` and `validation_steps` are used by `fit_generator` to decide the number of next() calls it need to make."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (num_train_sequences%batch_size) == 0:\n",
    "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
    "else:\n",
    "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
    "\n",
    "if (num_val_sequences%batch_size) == 0:\n",
    "    validation_steps = int(num_val_sequences/batch_size)\n",
    "else:\n",
    "    validation_steps = (num_val_sequences//batch_size) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now fit the model. This will start training the model and with the help of the checkpoints, you'll be able to save the model at the end of each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "Source path =  ../Project_data/train ; batch size = 16\n",
      "Source path =  ../Project_data/val ; batch size = 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/fastai/lib/python3.6/site-packages/ipykernel_launcher.py:21: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n",
      "/opt/conda/envs/fastai/lib/python3.6/site-packages/skimage/transform/_warps.py:105: UserWarning: The default mode, 'constant', will be changed to 'reflect' in skimage 0.15.\n",
      "  warn(\"The default mode, 'constant', will be changed to 'reflect' in \"\n",
      "/opt/conda/envs/fastai/lib/python3.6/site-packages/skimage/transform/_warps.py:110: UserWarning: Anti-aliasing will be enabled by default in skimage 0.15 to avoid aliasing artifacts when down-sampling images.\n",
      "  warn(\"Anti-aliasing will be enabled by default in skimage 0.15 to \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 7/42 [====>.........................] - ETA: 3:16 - loss: 1.6759 - categorical_accuracy: 0.2500"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/fastai/lib/python3.6/site-packages/ipykernel_launcher.py:54: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42/42 [==============================] - 220s 5s/step - loss: 1.3581 - categorical_accuracy: 0.4298 - val_loss: 1.0523 - val_categorical_accuracy: 0.6300\n",
      "\n",
      "Epoch 00001: saving model to model_init_conv2d_gru_2018-12-2801_12_09.769114/model-00001-1.35765-0.42986-1.05226-0.63000.h5\n",
      "Epoch 2/50\n",
      "42/42 [==============================] - 182s 4s/step - loss: 0.9076 - categorical_accuracy: 0.7112 - val_loss: 0.9603 - val_categorical_accuracy: 0.6400\n",
      "\n",
      "Epoch 00002: saving model to model_init_conv2d_gru_2018-12-2801_12_09.769114/model-00002-0.90525-0.71493-0.96028-0.64000.h5\n",
      "Epoch 3/50\n",
      "42/42 [==============================] - 178s 4s/step - loss: 0.6929 - categorical_accuracy: 0.7875 - val_loss: 0.7697 - val_categorical_accuracy: 0.7600\n",
      "\n",
      "Epoch 00003: saving model to model_init_conv2d_gru_2018-12-2801_12_09.769114/model-00003-0.68483-0.79035-0.76967-0.76000.h5\n",
      "Epoch 4/50\n",
      "42/42 [==============================] - 178s 4s/step - loss: 0.5194 - categorical_accuracy: 0.8623 - val_loss: 0.8599 - val_categorical_accuracy: 0.6500\n",
      "\n",
      "Epoch 00004: saving model to model_init_conv2d_gru_2018-12-2801_12_09.769114/model-00004-0.51622-0.86425-0.85988-0.65000.h5\n",
      "Epoch 5/50\n",
      "42/42 [==============================] - 193s 5s/step - loss: 0.4223 - categorical_accuracy: 0.9095 - val_loss: 0.7214 - val_categorical_accuracy: 0.6800\n",
      "\n",
      "Epoch 00005: saving model to model_init_conv2d_gru_2018-12-2801_12_09.769114/model-00005-0.41387-0.91403-0.72138-0.68000.h5\n",
      "Epoch 6/50\n",
      "42/42 [==============================] - 195s 5s/step - loss: 0.3119 - categorical_accuracy: 0.9438 - val_loss: 0.6731 - val_categorical_accuracy: 0.7700\n",
      "\n",
      "Epoch 00006: saving model to model_init_conv2d_gru_2018-12-2801_12_09.769114/model-00006-0.30024-0.94872-0.67312-0.77000.h5\n",
      "Epoch 7/50\n",
      "42/42 [==============================] - 193s 5s/step - loss: 0.2440 - categorical_accuracy: 0.9568 - val_loss: 0.6781 - val_categorical_accuracy: 0.6900\n",
      "\n",
      "Epoch 00007: saving model to model_init_conv2d_gru_2018-12-2801_12_09.769114/model-00007-0.23246-0.96380-0.67812-0.69000.h5\n",
      "Epoch 8/50\n",
      "42/42 [==============================] - 200s 5s/step - loss: 0.1909 - categorical_accuracy: 0.9706 - val_loss: 0.6528 - val_categorical_accuracy: 0.7600\n",
      "\n",
      "Epoch 00008: saving model to model_init_conv2d_gru_2018-12-2801_12_09.769114/model-00008-0.18229-0.97587-0.65279-0.76000.h5\n",
      "Epoch 9/50\n",
      "42/42 [==============================] - 173s 4s/step - loss: 0.1523 - categorical_accuracy: 0.9784 - val_loss: 0.6252 - val_categorical_accuracy: 0.7500\n",
      "\n",
      "Epoch 00009: saving model to model_init_conv2d_gru_2018-12-2801_12_09.769114/model-00009-0.14374-0.98190-0.62516-0.75000.h5\n",
      "Epoch 10/50\n",
      "42/42 [==============================] - 178s 4s/step - loss: 0.1185 - categorical_accuracy: 0.9829 - val_loss: 0.6211 - val_categorical_accuracy: 0.6800\n",
      "\n",
      "Epoch 00010: saving model to model_init_conv2d_gru_2018-12-2801_12_09.769114/model-00010-0.11052-0.98643-0.62113-0.68000.h5\n",
      "Epoch 11/50\n",
      "42/42 [==============================] - 175s 4s/step - loss: 0.1287 - categorical_accuracy: 0.9787 - val_loss: 0.6152 - val_categorical_accuracy: 0.7700\n",
      "\n",
      "Epoch 00011: saving model to model_init_conv2d_gru_2018-12-2801_12_09.769114/model-00011-0.10437-0.98793-0.61521-0.77000.h5\n",
      "Epoch 12/50\n",
      "42/42 [==============================] - 178s 4s/step - loss: 0.0932 - categorical_accuracy: 0.9903 - val_loss: 0.7005 - val_categorical_accuracy: 0.7400\n",
      "\n",
      "Epoch 00012: saving model to model_init_conv2d_gru_2018-12-2801_12_09.769114/model-00012-0.08534-0.99397-0.70046-0.74000.h5\n",
      "Epoch 13/50\n",
      "42/42 [==============================] - 176s 4s/step - loss: 0.1013 - categorical_accuracy: 0.9836 - val_loss: 0.6663 - val_categorical_accuracy: 0.7700\n",
      "\n",
      "Epoch 00013: saving model to model_init_conv2d_gru_2018-12-2801_12_09.769114/model-00013-0.08330-0.99095-0.66631-0.77000.h5\n",
      "Epoch 14/50\n",
      "42/42 [==============================] - 175s 4s/step - loss: 0.0797 - categorical_accuracy: 0.9918 - val_loss: 0.5641 - val_categorical_accuracy: 0.7900\n",
      "\n",
      "Epoch 00014: saving model to model_init_conv2d_gru_2018-12-2801_12_09.769114/model-00014-0.07598-0.99548-0.56409-0.79000.h5\n",
      "Epoch 15/50\n",
      "42/42 [==============================] - 176s 4s/step - loss: 0.0739 - categorical_accuracy: 0.9899 - val_loss: 0.6131 - val_categorical_accuracy: 0.7900\n",
      "\n",
      "Epoch 00015: saving model to model_init_conv2d_gru_2018-12-2801_12_09.769114/model-00015-0.06027-0.99548-0.61311-0.79000.h5\n",
      "Epoch 16/50\n",
      "42/42 [==============================] - 175s 4s/step - loss: 0.0719 - categorical_accuracy: 0.9884 - val_loss: 0.6375 - val_categorical_accuracy: 0.7700\n",
      "\n",
      "Epoch 00016: saving model to model_init_conv2d_gru_2018-12-2801_12_09.769114/model-00016-0.06038-0.99397-0.63746-0.77000.h5\n",
      "Epoch 17/50\n",
      "42/42 [==============================] - 176s 4s/step - loss: 0.0589 - categorical_accuracy: 0.9903 - val_loss: 0.5631 - val_categorical_accuracy: 0.7700\n",
      "\n",
      "Epoch 00017: saving model to model_init_conv2d_gru_2018-12-2801_12_09.769114/model-00017-0.05102-0.99397-0.56311-0.77000.h5\n",
      "Epoch 18/50\n",
      "42/42 [==============================] - 181s 4s/step - loss: 0.0591 - categorical_accuracy: 0.9933 - val_loss: 0.5711 - val_categorical_accuracy: 0.8000\n",
      "\n",
      "Epoch 00018: saving model to model_init_conv2d_gru_2018-12-2801_12_09.769114/model-00018-0.04888-0.99698-0.57106-0.80000.h5\n",
      "Epoch 19/50\n",
      "42/42 [==============================] - 175s 4s/step - loss: 0.0662 - categorical_accuracy: 0.9933 - val_loss: 0.5909 - val_categorical_accuracy: 0.7600\n",
      "\n",
      "Epoch 00019: saving model to model_init_conv2d_gru_2018-12-2801_12_09.769114/model-00019-0.04765-0.99698-0.59087-0.76000.h5\n",
      "Epoch 20/50\n",
      "42/42 [==============================] - 179s 4s/step - loss: 0.0531 - categorical_accuracy: 0.9933 - val_loss: 0.5575 - val_categorical_accuracy: 0.7700\n",
      "\n",
      "Epoch 00020: saving model to model_init_conv2d_gru_2018-12-2801_12_09.769114/model-00020-0.04385-0.99698-0.55751-0.77000.h5\n",
      "Epoch 21/50\n",
      "42/42 [==============================] - 180s 4s/step - loss: 0.0598 - categorical_accuracy: 0.9899 - val_loss: 0.5278 - val_categorical_accuracy: 0.8000\n",
      "\n",
      "Epoch 00021: saving model to model_init_conv2d_gru_2018-12-2801_12_09.769114/model-00021-0.04428-0.99548-0.52785-0.80000.h5\n",
      "Epoch 22/50\n",
      "42/42 [==============================] - 178s 4s/step - loss: 0.0393 - categorical_accuracy: 0.9966 - val_loss: 0.6183 - val_categorical_accuracy: 0.7500\n",
      "\n",
      "Epoch 00022: saving model to model_init_conv2d_gru_2018-12-2801_12_09.769114/model-00022-0.03506-0.99849-0.61829-0.75000.h5\n",
      "Epoch 23/50\n",
      "42/42 [==============================] - 179s 4s/step - loss: 0.0561 - categorical_accuracy: 0.9899 - val_loss: 0.6876 - val_categorical_accuracy: 0.7400\n",
      "\n",
      "Epoch 00023: saving model to model_init_conv2d_gru_2018-12-2801_12_09.769114/model-00023-0.03912-0.99548-0.68759-0.74000.h5\n",
      "Epoch 24/50\n",
      "42/42 [==============================] - 181s 4s/step - loss: 0.0629 - categorical_accuracy: 0.9918 - val_loss: 0.6802 - val_categorical_accuracy: 0.7500\n",
      "\n",
      "Epoch 00024: saving model to model_init_conv2d_gru_2018-12-2801_12_09.769114/model-00024-0.05288-0.99548-0.68015-0.75000.h5\n",
      "Epoch 25/50\n",
      "42/42 [==============================] - 178s 4s/step - loss: 0.0471 - categorical_accuracy: 0.9952 - val_loss: 0.6067 - val_categorical_accuracy: 0.8000\n",
      "\n",
      "Epoch 00025: saving model to model_init_conv2d_gru_2018-12-2801_12_09.769114/model-00025-0.03988-0.99698-0.60669-0.80000.h5\n",
      "Epoch 26/50\n",
      "42/42 [==============================] - 176s 4s/step - loss: 0.0464 - categorical_accuracy: 0.9952 - val_loss: 0.6221 - val_categorical_accuracy: 0.7900\n",
      "\n",
      "Epoch 00026: saving model to model_init_conv2d_gru_2018-12-2801_12_09.769114/model-00026-0.03906-0.99698-0.62214-0.79000.h5\n",
      "\n",
      "Epoch 00026: ReduceLROnPlateau reducing learning rate to 0.0019999999552965165.\n",
      "Epoch 27/50\n",
      "42/42 [==============================] - 179s 4s/step - loss: 0.0545 - categorical_accuracy: 0.9884 - val_loss: 0.5482 - val_categorical_accuracy: 0.8100\n",
      "\n",
      "Epoch 00027: saving model to model_init_conv2d_gru_2018-12-2801_12_09.769114/model-00027-0.04369-0.99397-0.54823-0.81000.h5\n",
      "Epoch 28/50\n",
      "42/42 [==============================] - 181s 4s/step - loss: 0.0362 - categorical_accuracy: 0.9966 - val_loss: 0.5913 - val_categorical_accuracy: 0.7800\n",
      "\n",
      "Epoch 00028: saving model to model_init_conv2d_gru_2018-12-2801_12_09.769114/model-00028-0.03131-0.99849-0.59125-0.78000.h5\n",
      "Epoch 29/50\n",
      "42/42 [==============================] - 178s 4s/step - loss: 0.0322 - categorical_accuracy: 0.9966 - val_loss: 0.5669 - val_categorical_accuracy: 0.7900\n",
      "\n",
      "Epoch 00029: saving model to model_init_conv2d_gru_2018-12-2801_12_09.769114/model-00029-0.02790-0.99849-0.56688-0.79000.h5\n",
      "Epoch 30/50\n",
      "42/42 [==============================] - 178s 4s/step - loss: 0.0617 - categorical_accuracy: 0.9899 - val_loss: 0.5568 - val_categorical_accuracy: 0.7900\n",
      "\n",
      "Epoch 00030: saving model to model_init_conv2d_gru_2018-12-2801_12_09.769114/model-00030-0.03980-0.99548-0.55681-0.79000.h5\n",
      "Epoch 31/50\n",
      "42/42 [==============================] - 178s 4s/step - loss: 0.0298 - categorical_accuracy: 0.9966 - val_loss: 0.5705 - val_categorical_accuracy: 0.7900\n",
      "\n",
      "Epoch 00031: saving model to model_init_conv2d_gru_2018-12-2801_12_09.769114/model-00031-0.02638-0.99849-0.57054-0.79000.h5\n",
      "\n",
      "Epoch 00031: ReduceLROnPlateau reducing learning rate to 0.001.\n",
      "Epoch 32/50\n",
      "42/42 [==============================] - 177s 4s/step - loss: 0.0363 - categorical_accuracy: 0.9933 - val_loss: 0.5858 - val_categorical_accuracy: 0.7800\n",
      "\n",
      "Epoch 00032: saving model to model_init_conv2d_gru_2018-12-2801_12_09.769114/model-00032-0.02898-0.99698-0.58585-0.78000.h5\n",
      "Epoch 33/50\n",
      "42/42 [==============================] - 175s 4s/step - loss: 0.0380 - categorical_accuracy: 0.9899 - val_loss: 0.5973 - val_categorical_accuracy: 0.7800\n",
      "\n",
      "Epoch 00033: saving model to model_init_conv2d_gru_2018-12-2801_12_09.769114/model-00033-0.02923-0.99548-0.59731-0.78000.h5\n",
      "Epoch 34/50\n",
      "42/42 [==============================] - 177s 4s/step - loss: 0.0395 - categorical_accuracy: 0.9933 - val_loss: 0.5535 - val_categorical_accuracy: 0.7800\n",
      "\n",
      "Epoch 00034: saving model to model_init_conv2d_gru_2018-12-2801_12_09.769114/model-00034-0.02971-0.99698-0.55345-0.78000.h5\n",
      "Epoch 35/50\n",
      "42/42 [==============================] - 178s 4s/step - loss: 0.0323 - categorical_accuracy: 0.9966 - val_loss: 0.5383 - val_categorical_accuracy: 0.7900\n",
      "\n",
      "Epoch 00035: saving model to model_init_conv2d_gru_2018-12-2801_12_09.769114/model-00035-0.02596-0.99849-0.53834-0.79000.h5\n",
      "Epoch 36/50\n",
      "42/42 [==============================] - 175s 4s/step - loss: 0.0330 - categorical_accuracy: 0.9933 - val_loss: 0.5287 - val_categorical_accuracy: 0.7900\n",
      "\n",
      "Epoch 00036: saving model to model_init_conv2d_gru_2018-12-2801_12_09.769114/model-00036-0.02607-0.99698-0.52870-0.79000.h5\n",
      "\n",
      "Epoch 00036: ReduceLROnPlateau reducing learning rate to 0.001.\n",
      "Epoch 37/50\n",
      "42/42 [==============================] - 178s 4s/step - loss: 0.0231 - categorical_accuracy: 1.0000 - val_loss: 0.5860 - val_categorical_accuracy: 0.7600\n",
      "\n",
      "Epoch 00037: saving model to model_init_conv2d_gru_2018-12-2801_12_09.769114/model-00037-0.02148-1.00000-0.58601-0.76000.h5\n",
      "Epoch 38/50\n",
      "42/42 [==============================] - 178s 4s/step - loss: 0.0322 - categorical_accuracy: 0.9966 - val_loss: 0.5472 - val_categorical_accuracy: 0.7800\n",
      "\n",
      "Epoch 00038: saving model to model_init_conv2d_gru_2018-12-2801_12_09.769114/model-00038-0.02519-0.99849-0.54724-0.78000.h5\n",
      "Epoch 39/50\n",
      "42/42 [==============================] - 178s 4s/step - loss: 0.0233 - categorical_accuracy: 1.0000 - val_loss: 0.5555 - val_categorical_accuracy: 0.7900\n",
      "\n",
      "Epoch 00039: saving model to model_init_conv2d_gru_2018-12-2801_12_09.769114/model-00039-0.02106-1.00000-0.55552-0.79000.h5\n",
      "Epoch 40/50\n",
      "42/42 [==============================] - 174s 4s/step - loss: 0.0423 - categorical_accuracy: 0.9866 - val_loss: 0.5726 - val_categorical_accuracy: 0.7800\n",
      "\n",
      "Epoch 00040: saving model to model_init_conv2d_gru_2018-12-2801_12_09.769114/model-00040-0.02933-0.99397-0.57261-0.78000.h5\n",
      "Epoch 41/50\n",
      "42/42 [==============================] - 178s 4s/step - loss: 0.0399 - categorical_accuracy: 0.9899 - val_loss: 0.5332 - val_categorical_accuracy: 0.8100\n",
      "\n",
      "Epoch 00041: saving model to model_init_conv2d_gru_2018-12-2801_12_09.769114/model-00041-0.02815-0.99548-0.53316-0.81000.h5\n",
      "\n",
      "Epoch 00041: ReduceLROnPlateau reducing learning rate to 0.001.\n",
      "Epoch 42/50\n",
      "42/42 [==============================] - 178s 4s/step - loss: 0.0282 - categorical_accuracy: 0.9966 - val_loss: 0.5537 - val_categorical_accuracy: 0.7900\n",
      "\n",
      "Epoch 00042: saving model to model_init_conv2d_gru_2018-12-2801_12_09.769114/model-00042-0.02313-0.99849-0.55373-0.79000.h5\n",
      "Epoch 43/50\n",
      "42/42 [==============================] - 175s 4s/step - loss: 0.0294 - categorical_accuracy: 0.9966 - val_loss: 0.5707 - val_categorical_accuracy: 0.7800\n",
      "\n",
      "Epoch 00043: saving model to model_init_conv2d_gru_2018-12-2801_12_09.769114/model-00043-0.02324-0.99849-0.57073-0.78000.h5\n",
      "Epoch 44/50\n",
      "42/42 [==============================] - 173s 4s/step - loss: 0.0313 - categorical_accuracy: 0.9933 - val_loss: 0.5408 - val_categorical_accuracy: 0.8000\n",
      "\n",
      "Epoch 00044: saving model to model_init_conv2d_gru_2018-12-2801_12_09.769114/model-00044-0.02387-0.99698-0.54078-0.80000.h5\n",
      "Epoch 45/50\n",
      "42/42 [==============================] - 177s 4s/step - loss: 0.0380 - categorical_accuracy: 0.9933 - val_loss: 0.5595 - val_categorical_accuracy: 0.7700\n",
      "\n",
      "Epoch 00045: saving model to model_init_conv2d_gru_2018-12-2801_12_09.769114/model-00045-0.02685-0.99698-0.55953-0.77000.h5\n",
      "Epoch 46/50\n",
      "42/42 [==============================] - 179s 4s/step - loss: 0.0341 - categorical_accuracy: 0.9899 - val_loss: 0.5829 - val_categorical_accuracy: 0.7900\n",
      "\n",
      "Epoch 00046: saving model to model_init_conv2d_gru_2018-12-2801_12_09.769114/model-00046-0.02509-0.99548-0.58293-0.79000.h5\n",
      "\n",
      "Epoch 00046: ReduceLROnPlateau reducing learning rate to 0.001.\n",
      "Epoch 47/50\n",
      "42/42 [==============================] - 180s 4s/step - loss: 0.0220 - categorical_accuracy: 1.0000 - val_loss: 0.5795 - val_categorical_accuracy: 0.7600\n",
      "\n",
      "Epoch 00047: saving model to model_init_conv2d_gru_2018-12-2801_12_09.769114/model-00047-0.01943-1.00000-0.57951-0.76000.h5\n",
      "Epoch 48/50\n",
      "42/42 [==============================] - 177s 4s/step - loss: 0.0297 - categorical_accuracy: 0.9966 - val_loss: 0.5830 - val_categorical_accuracy: 0.7900\n",
      "\n",
      "Epoch 00048: saving model to model_init_conv2d_gru_2018-12-2801_12_09.769114/model-00048-0.02274-0.99849-0.58298-0.79000.h5\n",
      "Epoch 49/50\n",
      "42/42 [==============================] - 179s 4s/step - loss: 0.0353 - categorical_accuracy: 0.9899 - val_loss: 0.5682 - val_categorical_accuracy: 0.8000\n",
      "\n",
      "Epoch 00049: saving model to model_init_conv2d_gru_2018-12-2801_12_09.769114/model-00049-0.02533-0.99548-0.56822-0.80000.h5\n",
      "Epoch 50/50\n",
      "42/42 [==============================] - 181s 4s/step - loss: 0.0355 - categorical_accuracy: 0.9933 - val_loss: 0.5732 - val_categorical_accuracy: 0.8100\n",
      "\n",
      "Epoch 00050: saving model to model_init_conv2d_gru_2018-12-2801_12_09.769114/model-00050-0.02526-0.99698-0.57317-0.81000.h5\n"
     ]
    }
   ],
   "source": [
    "conv2d_gru_model = model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "                              callbacks=callbacks_list, validation_data=val_generator, \n",
    "                              validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building graphs #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(conv2d_gru_model.history[\"categorical_accuracy\"])\n",
    "plt.plot(conv2d_gru_model.history[\"val_categorical_accuracy\"])\n",
    "plt.xlabel(\"EPOCHS\")\n",
    "plt.ylabel(\"Percentage\")\n",
    "plt.ylim(0.0,1.0)\n",
    "plt.title(\"Conv2D+GRU Model\")\n",
    "plt.legend([\"Train\",\"Test\"],loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(conv2d_gru_model.history[\"loss\"])\n",
    "plt.plot(conv2d_gru_model.history[\"val_loss\"])\n",
    "plt.xlabel(\"EPOCHS\")\n",
    "plt.ylabel(\"LOSS\")\n",
    "plt.ylim(0.0,3.0)\n",
    "plt.title(\"Conv2D+GRU Model (LOSS)\")\n",
    "plt.legend([\"Train\",\"Testing\"],loc=\"upper right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking the EPOCH index where maximum validation accuracy was achieved ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum Validation Accuracy = 0.810000 achieved at epoch idx = 26\n"
     ]
    }
   ],
   "source": [
    "print(\"Maximum Validation Accuracy = %f achieved at epoch iteration = %d\" %(np.array(conv2d_gru_model.history[\"val_categorical_accuracy\"]).max(),np.array(conv2d_gru_model.history[\"val_categorical_accuracy\"]).argmax() + 1))\n",
    "sel_model_idx = np.array(conv2d_gru_model.history[\"val_categorical_accuracy\"]).argmax() + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Selecting h5 file for loading model ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected model name : model_init_conv2d_gru_2018-12-2801_12_09.769114/model-00026-0.03906-0.99698-0.62214-0.79000.h5\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "search_string = \"model-0*\" + str(sel_model_idx) + \"-[a-z0-9\\-\\.]*\"\n",
    "selected_model_name = model_name + list(filter(lambda x: re.search(search_string,x),os.listdir(model_name)))[0]\n",
    "print(\"Selected model name : %s\" %(selected_model_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading model weights from h5 file ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import clone_model\n",
    "selected_model = clone_model(model)\n",
    "selected_model.load_weights(selected_model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Performing prediction on validation data ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source path =  ../../../Project_data/val ; batch size = 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/fastai/lib/python3.6/site-packages/ipykernel_launcher.py:21: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n",
      "/opt/conda/envs/fastai/lib/python3.6/site-packages/skimage/transform/_warps.py:105: UserWarning: The default mode, 'constant', will be changed to 'reflect' in skimage 0.15.\n",
      "  warn(\"The default mode, 'constant', will be changed to 'reflect' in \"\n",
      "/opt/conda/envs/fastai/lib/python3.6/site-packages/skimage/transform/_warps.py:110: UserWarning: Anti-aliasing will be enabled by default in skimage 0.15 to avoid aliasing artifacts when down-sampling images.\n",
      "  warn(\"Anti-aliasing will be enabled by default in skimage 0.15 to \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 0s 21ms/step\n",
      "16/16 [==============================] - 0s 3ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "16/16 [==============================] - 0s 3ms/step\n",
      "16/16 [==============================] - 0s 3ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/fastai/lib/python3.6/site-packages/ipykernel_launcher.py:54: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 10ms/step\n"
     ]
    }
   ],
   "source": [
    "testing_val_generator = generator(val_path,\n",
    "                          val_data, \n",
    "                          batch_size=batch_size,\n",
    "                          transform_size=transform_size,\n",
    "                          frame_selection_list=frame_selection_list,\n",
    "                          process_input_func=preprocess_input_func,\n",
    "                          base_model=base_model)\n",
    "result = None\n",
    "for x in range(validation_steps):\n",
    "    x_val,y_val = next(testing_val_generator)\n",
    "    p_val = selected_model.predict(x_val,batch_size=batch_size,verbose=1)\n",
    "    if x == 0:\n",
    "        result = np.vstack((y_val.argmax(axis=1),p_val.argmax(axis=1)))\n",
    "    else:\n",
    "        result = np.hstack((result,np.vstack((y_val.argmax(axis=1),p_val.argmax(axis=1)))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Consolidating Results ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.DataFrame(np.transpose(result),columns=[\"Actual\",\"Predicted\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix,classification_report,accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-30T13:42:10.469641Z",
     "start_time": "2018-12-30T13:42:10.448625Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Predicted_Left</th>\n",
       "      <th>Predicted_Right</th>\n",
       "      <th>Predicted_Stop</th>\n",
       "      <th>Predicted_Down</th>\n",
       "      <th>Predicted_Up</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Actual_Left</th>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual_Right</th>\n",
       "      <td>5</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual_Stop</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual_Down</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual_Up</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Predicted_Left  Predicted_Right  Predicted_Stop  Predicted_Down  \\\n",
       "Actual_Left               10                6               2               0   \n",
       "Actual_Right               5               15               1               0   \n",
       "Actual_Stop                0                1              20               1   \n",
       "Actual_Down                0                0               0              21   \n",
       "Actual_Up                  1                0               3               0   \n",
       "\n",
       "              Predicted_Up  \n",
       "Actual_Left              0  \n",
       "Actual_Right             2  \n",
       "Actual_Stop              0  \n",
       "Actual_Down              0  \n",
       "Actual_Up               12  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cf_np = confusion_matrix(result.Actual,result.Predicted,labels=range(0,5))\n",
    "cf_df = pd.DataFrame(cf_np,columns=[\"Predicted_\" + str(x) for x in [\"Left\",\"Right\",\"Stop\",\"Down\",\"Up\"]],index=[\"Actual_\" + str(x) for x in [\"Left\",\"Right\",\"Stop\",\"Down\",\"Up\"]])\n",
    "cf_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cummalative Accuracy Score : 0.780000\n",
      "Cummalative Categorical Accuracy : 0.810000\n"
     ]
    }
   ],
   "source": [
    "print(\"Cummalative Accuracy Score : %f\" %(accuracy_score(result.Actual,result.Predicted)))\n",
    "print(\"Cummalative Categorical Accuracy : %f\" %(0.8100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
