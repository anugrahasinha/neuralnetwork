{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "843a853a75c0d84044dc32591f0451d67c7386cc"
   },
   "source": [
    "# Gesture Recognition (Using Conv2D + LSTM technique)\n",
    "\n",
    "*Author : Anugraha Sinha*\n",
    "\n",
    "*Email : anugraha[dot]sinha[at]gmail[dot]com*\n",
    "\n",
    "In this case, we will be using a CNN2D + GRU (Gated Recurrent Unit) Neural Network for evaluating geatures in videos. \n",
    "The data provided as following features\n",
    "1. 30 Video frames in each video\n",
    "2. Each frame - 3 Channel (RGB)\n",
    "3. 5 kinds of gestures (Up, Down, Right, Left, Stop)\n",
    "\n",
    "The objective is to be fullfilled for a hypothetical Smart TV company which is trying to incorporate geature recognition in its product where in the gestures mean following things\n",
    "\n",
    "Gesture   |       Left       |    Right    |    Stop      |    Down     |     Up\n",
    "----------| -----------------|-------------|--------------|-------------|-----------\n",
    "Objectives| Previous Channel | Next Channel| Stop Playing | Volume Down | Volume Up\n",
    "Y Value (Integer) | 0 | 1 | 2 | 3 | 4 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "8f6234a0b348a4b7ec0b06b2a81e7d33697455bd"
   },
   "source": [
    "#### Library Imports ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-24T10:02:45.291268Z",
     "start_time": "2018-12-24T10:02:16.166997Z"
    },
    "_uuid": "6f394d53a91d25a807c26ccc6b3cc19868c88980"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from scipy.misc import imread, imresize\n",
    "import datetime\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "566527b4b778d6cf1980de78012904301c1cc198"
   },
   "source": [
    "##### We set the random seed so that the results don't vary drastically.#####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-24T10:03:12.325679Z",
     "start_time": "2018-12-24T10:02:45.295206Z"
    },
    "_uuid": "408cf95f119af2a2f232af2c079fd77f19d5cb5b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(30)\n",
    "import random as rn\n",
    "rn.seed(30)\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "tf.set_random_seed(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d39a9875807a84a724594cedc2ca430494ed5a43"
   },
   "source": [
    "##### We set the train_doc and val_doc location #####\n",
    "*Note : Please set this location as per your environment.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-24T10:03:12.439253Z",
     "start_time": "2018-12-24T10:03:12.328631Z"
    },
    "_uuid": "6576b99114c81c01d98f2597a55f059c35608e7e"
   },
   "outputs": [],
   "source": [
    "train_doc = np.random.permutation(open('../input/data/Project_data/train.csv').readlines())\n",
    "val_doc = np.random.permutation(open('../input/data/Project_data/val.csv').readlines())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b64b6929587967c0ac1667443de90e4768773a7e"
   },
   "source": [
    "#### Image Processor ####\n",
    "The objective of this function is:\n",
    "Resize the images as per *transform_size* given by user, or take it as (120,120)\n",
    "\n",
    "**Reason**\n",
    "\n",
    "In the training data we have 2 types of video, where video frame sizes are as *(360,360)* and *(160,120)* Therefore to keep things consistent, we build a image processor which perform cropping of the image.\n",
    "\n",
    "**Note**\n",
    "\n",
    "Cropping is done as a centered image, i.e., we take the center as the reference and crop the images from both sides. For (360,360) we can use skimage.transform.resize function. However, for *(160,120)* we will have to some manual processing to crop images by the center."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-24T10:03:24.517873Z",
     "start_time": "2018-12-24T10:03:12.444250Z"
    },
    "_uuid": "71fa5f175060fd0291e2709f91a7ded7e9f8dc0f"
   },
   "outputs": [],
   "source": [
    "from skimage.transform import resize\n",
    "def image_processor(orig_image,transform_size=(120,120)):\n",
    "    # If we have an image of (360,360,3) then we use resize function.\n",
    "    # If we have an image of \n",
    "    new_image = orig_image\n",
    "    if orig_image.shape == (360,360,3):\n",
    "        new_image = resize(orig_image,transform_size)\n",
    "    else:\n",
    "        start_row = (orig_image.shape[0] - transform_size[0])//2\n",
    "        end_row = start_row + transform_size[0]\n",
    "        \n",
    "        start_col = (orig_image.shape[1] - transform_size[1])//2\n",
    "        end_col = start_col + transform_size[1]\n",
    "        new_image = orig_image[start_row:end_row,start_col:end_col,:]\n",
    "    #new_image = resize(orig_image,transform_size)\n",
    "    return(new_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "9564495fc61e44afa20d6c28125763f842c17db1"
   },
   "source": [
    "#### Sequence Generator ####\n",
    "This function provides a list of frame sequences we would like to use training\n",
    "\n",
    "Arguments\n",
    "\n",
    "a) choiceoflist\n",
    "\n",
    "\n",
    "choicelist | 0 | 1 | 2\n",
    "-----------|---|---|---\n",
    "Return list type | range(0,30,1) | range(0,30,2) | [0,1,2,3,4,5,6,9,12,15,18,21,24,25,26,27,28,29]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_uuid": "602daa25389460d1da1c4333799dd58d875bdbfb"
   },
   "outputs": [],
   "source": [
    "def getframeselectionlist(choiceoflist=0):\n",
    "    if choiceoflist==0:\n",
    "        return [frame for frame in range(0,30,1)] # Returns 100% of frames, number of frames=30\n",
    "    elif choiceoflist==1:\n",
    "        return [frame for frame in range(0,30,2)] # returns 50% of frames, number of frames=15\n",
    "    elif choiceoflist==2:\n",
    "        \n",
    "        # For this we are taking first 5 frame+skip frame sequence+last 5 frame of the sequence\n",
    "        \n",
    "        frame_sequence=[]\n",
    "        \n",
    "        startframesequence=[0,1,2,3,4,5]\n",
    "        endframesequence=[25,26,27,28,29]\n",
    "        skip_sequence=3  #\n",
    "        middleframesequnce=[k for k in range(6,25,skip_sequence)]\n",
    "        \n",
    "        frame_sequence.extend(startframesequence)\n",
    "        frame_sequence.extend(middleframesequnce)\n",
    "        frame_sequence.extend(endframesequence)\n",
    "        \n",
    "        return frame_sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f2b79c140a0d06a07999494a5c2cf89224073c44"
   },
   "source": [
    "### Generator ###\n",
    "This is heart of complete training process. It pumps batched data to network during learning and prediction both. The function description is given below\n",
    "\n",
    "**Arguments**\n",
    "\n",
    "1. Source Path - Directory path to be considered for reading video/images frames\n",
    "2. folder_list - Lines from the train_doc we read above.\n",
    "3. batch_size - The batch_size we want to select.\n",
    "4. transform_size - The image transformation size we  (Default - (120,120)\n",
    "5. frame_selection_list - frame_list obtained from frame_generator (Default - range(30))\n",
    "6. process_input_func - To be provided in case CNN2D+RNN type modelling being done\n",
    "7. base_model - To be provided in case CNN2D+RNN type modelling being done.\n",
    "\n",
    "**Working**\n",
    "* Case when CNN3D modelling being done\n",
    "\n",
    "In this case, for each batch (according to batch size), we build \n",
    "\n",
    "1. **batch_data** = *(batch_size, number_of_frames,image_size_x,image_size_y,n_channels)*\n",
    "2. We normalize each channel (RGB) by dividing the pixel value with 255.\n",
    "\n",
    "\n",
    "* Case when CNN2D+RNN modelling being done (RNN can be any of SimpleRNN/LSTM/GRU)\n",
    "\n",
    "In this case, for each batch (according to batch size), we build\n",
    "\n",
    "1. **batch_data** = *(batch_size, number_of_frames,image_size_x,image_size_y,n_channels)*\n",
    "2. reshape batch data as **batch_data.reshape(batch_size * number_of_frames , image_size_x , image_size_y , n_channels)**.\n",
    "3. Above reshaped numpy array is sent to *process_input_func* of the pre-learned CNN2D function. This will produce modified image vector as per pre-learned CNN2D function (like VGG19/VGG16/etc.)\n",
    "4. After *process_input_func* we reshape again to *(batch_size, number_of_frames, outputs from CNN2D vector)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-24T10:03:24.551099Z",
     "start_time": "2018-12-24T10:03:24.517873Z"
    },
    "_uuid": "82cc91d7e0a8bd6b7a2bf54aa9264365e900af69"
   },
   "outputs": [],
   "source": [
    "def generator(source_path,\n",
    "              folder_list,\n",
    "              batch_size,\n",
    "              transform_size = (120,120),\n",
    "              frame_selection_list = range(30),\n",
    "              process_input_func=None,\n",
    "              base_model = None):\n",
    "    print( 'Source path = ', source_path, '; batch size =', batch_size)\n",
    "    img_idx = frame_selection_list\n",
    "    channels = 3\n",
    "    while True:\n",
    "        t = np.random.permutation(folder_list)\n",
    "        num_batches = int(len(t) / batch_size) if len(t) % batch_size == 0 else (len(t) // batch_size)\n",
    "        for batch in range(num_batches): # we iterate over the number of batches\n",
    "            batch_data = np.zeros((batch_size,len(frame_selection_list),transform_size[0],transform_size[1],channels)) # x is the number of images you use for each video, (y,z) is the final size of the input images and 3 is the number of channels RGB\n",
    "            batch_labels = np.zeros((batch_size,5)) # batch_labels is the one hot representation of the output\n",
    "            for folder in range(batch_size): # iterate over the batch_size\n",
    "                imgs = os.listdir(source_path+'/'+ t[folder + (batch*batch_size)].split(';')[0]) # read all the images in the folder\n",
    "                #print(t[folder + (batch*batch_size)].split(';')[0])\n",
    "                for idx,item in enumerate(img_idx): #  Iterate iver the frames/images of a folder to read them in\n",
    "                    image = imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float64)\n",
    "                    #crop the images and resize them. Note that the images are of 2 different shape \n",
    "                    #and the conv3D will throw error if the inputs in a batch have different shapes\n",
    "                    image = image_processor(image,transform_size)\n",
    "                    if base_model:\n",
    "                        # This is when we are using a pre-learned CNN2D network\n",
    "                        image = process_input_func(image)\n",
    "                        \n",
    "                    else:\n",
    "                        # This is when we are building a Conv3D network\n",
    "                        image = image/255\n",
    "\n",
    "                    batch_data[folder,idx,:,:,0] = image[:,:,0]\n",
    "                    batch_data[folder,idx,:,:,1] = image[:,:,1]\n",
    "                    batch_data[folder,idx,:,:,2] = image[:,:,2]\n",
    "                    \n",
    "                batch_labels[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n",
    "            if base_model:\n",
    "                s1 = batch_data.shape\n",
    "                n1 = base_model.predict(batch_data.reshape(s1[0]*s1[1],s1[2],s1[3],s1[4]))\n",
    "                s2 = n1.shape\n",
    "                batch_data = n1.reshape(s1[0],s1[1],s2[1]*s2[2]*s2[3])\n",
    "            yield batch_data, batch_labels #you yield the batch_data and the batch_labels, remember what does yield do\n",
    "        \n",
    "        # write the code for the remaining data points which are left after full batches\n",
    "        # remaining data #\n",
    "        if len(t) % batch_size != 0:                                      # Execute only if, we need to\n",
    "            batch_data = np.zeros((len(t) % batch_size,len(frame_selection_list),transform_size[0],transform_size[1],channels))     # fix the last batch size\n",
    "            batch_labels = np.zeros((len(t) % batch_size,5))              # Similarly, for labels\n",
    "            for v_idx,folder in enumerate(t[(num_batches*batch_size):]):\n",
    "                #print(folder.split(';')[0])\n",
    "                imgs = os.listdir(source_path+\"/\"+folder.split(';')[0])\n",
    "                for idx, item in enumerate(img_idx):\n",
    "                    image = imread(source_path+'/'+folder.split(';')[0]+\"/\"+imgs[item]).astype(np.float64)\n",
    "                    image = image_processor(image,transform_size)\n",
    "\n",
    "                    if base_model:\n",
    "                        # This is when we are using a pre-learned CNN2D network\n",
    "                        image = process_input_func(image)\n",
    "                    else:\n",
    "                        # This is when we are building a Conv3D network\n",
    "                        image = image/255\n",
    "\n",
    "                    batch_data[v_idx,idx,:,:,0] = image[:,:,0]\n",
    "                    batch_data[v_idx,idx,:,:,1] = image[:,:,1]\n",
    "                    batch_data[v_idx,idx,:,:,0] = image[:,:,2]\n",
    "                batch_labels[v_idx,int(folder.split(\";\")[2])] = 1\n",
    "            if base_model:\n",
    "                s1 = batch_data.shape\n",
    "                n1 = base_model.predict(batch_data.reshape(s1[0]*s1[1],s1[2],s1[3],s1[4]))\n",
    "                s2 = n1.shape\n",
    "                batch_data = n1.reshape(s1[0],s1[1],s2[1]*s2[2]*s2[3])\n",
    "            yield batch_data,batch_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "42780b408d82023d7608c52b06e9dc0eb60f827c"
   },
   "source": [
    "Recording *current_date-time* and fixing training data and validation data folder paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-24T10:03:24.621378Z",
     "start_time": "2018-12-24T10:03:24.554098Z"
    },
    "_uuid": "109b421d3b5b9ebffed2a5f7fb9265142d64c92e"
   },
   "outputs": [],
   "source": [
    "curr_dt_time = datetime.datetime.now()\n",
    "train_path = '../input/data/Project_data/train'\n",
    "val_path = '../input/data/Project_data/val'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f6fa3f1bb86ec4b8aef3ba071eb524b0018becf0"
   },
   "source": [
    "## Model\n",
    "Setting up base information for model building process.\n",
    "\n",
    "*IMPORTANT*\n",
    "We are using VGG19 pre-trained model for CONV2D modelling.\n",
    "\n",
    "If this code is CONV3D, then even though we are building the *base_model* and *preprocess_input_func* variables, but we will not use them further in the code.\n",
    "\n",
    "If this code is CONV2D+RNN type modelling, then we will use *base_model* and *preprocess_input_func* variables further in the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-24T10:06:14.246527Z",
     "start_time": "2018-12-24T10:06:14.241521Z"
    },
    "_uuid": "3f75d9794ae2641403c78e1e44c6339f5e1873f0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "80142336/80134624 [==============================] - 1s 0us/step\n"
     ]
    }
   ],
   "source": [
    "train_data = train_doc\n",
    "val_data = val_doc\n",
    "transform_size = (120,120)\n",
    "batch_size = 16\n",
    "num_epochs = 40\n",
    "frame_selection_list = getframeselectionlist(choiceoflist=0)\n",
    "num_train_sequences = len(train_data)\n",
    "num_val_sequences = len(val_data)\n",
    "model_name = \"model_init_conv2d_lstm\"\n",
    "\n",
    "from keras.applications.vgg19 import VGG19 as base_conv2d_model\n",
    "from keras.applications.vgg19 import preprocess_input\n",
    "\n",
    "base_model = base_conv2d_model(weights=\"imagenet\",include_top=False)\n",
    "preprocess_input_func = preprocess_input\n",
    "\n",
    "os.environ.putenv(\"HDF5_USE_FILE_LOCKING\",\"FALSE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c36a9290f82dd9464886772d53e9f5dd44354233"
   },
   "source": [
    "#### Building Neural Network Model Architecture ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_uuid": "a3f82555137fd2a210dca2881d216911565fd0fa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source path =  ../input/data/Project_data/train ; batch size = 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:21: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n",
      "/opt/conda/lib/python3.6/site-packages/skimage/transform/_warps.py:84: UserWarning: The default mode, 'constant', will be changed to 'reflect' in skimage 0.15.\n",
      "  warn(\"The default mode, 'constant', will be changed to 'reflect' in \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape = (16, 30, 4608)\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, GRU, Flatten, TimeDistributed, Flatten, BatchNormalization, Activation, Dropout, Bidirectional, SimpleRNN, LSTM, GRU\n",
    "from keras.layers.convolutional import Conv3D, MaxPooling3D\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras import optimizers\n",
    "from keras.regularizers import l2\n",
    "\n",
    "input_sample = next(generator(train_path,\n",
    "                              train_data, \n",
    "                              batch_size=batch_size,\n",
    "                              transform_size=transform_size,\n",
    "                              frame_selection_list=frame_selection_list,\n",
    "                              process_input_func=preprocess_input_func,\n",
    "                              base_model=base_model))\n",
    "print(\"Input shape = %s\" %(str(input_sample[0].shape)))\n",
    "model = Sequential()\n",
    "model.add(LSTM(64,\n",
    "              input_shape=(input_sample[0].shape[1],input_sample[0].shape[2]),\n",
    "              dropout=0.5))\n",
    "\n",
    "model.add(Dense(input_sample[1].shape[1], activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f67c11894be704ea008c8eba6967762d829ff694"
   },
   "source": [
    "#### Setting up Neural Network configurations ####\n",
    "\n",
    "Optimizers\n",
    "loss function and metrics to be monitored.\n",
    "\n",
    "Watch out for model summary for verifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_uuid": "cfe23bcc9041c56c0693bef2d8aade7e4b15c4ab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 64)                1196288   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 5)                 325       \n",
      "=================================================================\n",
      "Total params: 1,196,613\n",
      "Trainable params: 1,196,613\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "optimizer = optimizers.SGD(lr=0.01)\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "print (model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "3b59bd91aed4e81b80c0992337127cc61a917583"
   },
   "source": [
    "#### Building up generator objects ####\n",
    "\n",
    "train_generator for training data\n",
    "\n",
    "val_generator for validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-24T10:05:22.311781Z",
     "start_time": "2018-12-24T10:05:22.306781Z"
    },
    "_uuid": "5d022d330951dd6e82be9d3d441643653be389d4"
   },
   "outputs": [],
   "source": [
    "train_generator = generator(train_path,\n",
    "                            train_data, \n",
    "                            batch_size=batch_size,\n",
    "                            transform_size=transform_size,\n",
    "                            frame_selection_list=frame_selection_list,\n",
    "                            process_input_func=preprocess_input_func,\n",
    "                            base_model=base_model)\n",
    "val_generator = generator(val_path,\n",
    "                          val_data, \n",
    "                          batch_size=batch_size,\n",
    "                          transform_size=transform_size,\n",
    "                          frame_selection_list=frame_selection_list,\n",
    "                          process_input_func=preprocess_input_func,\n",
    "                          base_model=base_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a80a157cc3eba05229e078c928d4ee9010c84de2"
   },
   "source": [
    "#### Setting up extra parameter for Neural Network ####\n",
    "\n",
    "* ModelCheckPoint\n",
    "\n",
    "We build modelcheckpoint to save models after every epoch, so that we can refer to models to best model later on for final testing.\n",
    "* ReduceLROnPlateau\n",
    "\n",
    "If the **val_loss** value stops improving after **patience** number of epochs, we reduce the learning rate so as to take smaller steps down the gradient route for raeaching global minimum and avoid getting stuch at local minimum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-24T10:05:28.669954Z",
     "start_time": "2018-12-24T10:05:28.660949Z"
    },
    "_uuid": "4f36e536d224ba57ebcf80effb975c97dfca2218"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_init_conv2d_lstm_2018-12-3003_00_58.183252/\n"
     ]
    }
   ],
   "source": [
    "model_name = model_name + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n",
    "    \n",
    "if not os.path.exists(model_name):\n",
    "    os.mkdir(model_name)\n",
    "        \n",
    "filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
    "\n",
    "LR = ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n",
    "                       patience=5, min_lr=0.001,verbose=1)\n",
    "\n",
    "callbacks_list = [checkpoint, LR]\n",
    "#callbacks_list = [checkpoint]\n",
    "#callbacks_list = []\n",
    "print(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c730c915920a8aa86f568dbec6eebe2c894850dc"
   },
   "source": [
    "The `steps_per_epoch` and `validation_steps` are used by `fit_generator` to decide the number of next() calls it need to make."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-24T10:06:22.943538Z",
     "start_time": "2018-12-24T10:06:22.938534Z"
    },
    "_uuid": "3255cc9a0b14551dff24b222520656658c4a523e"
   },
   "outputs": [],
   "source": [
    "if (num_train_sequences%batch_size) == 0:\n",
    "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
    "else:\n",
    "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
    "\n",
    "if (num_val_sequences%batch_size) == 0:\n",
    "    validation_steps = int(num_val_sequences/batch_size)\n",
    "else:\n",
    "    validation_steps = (num_val_sequences//batch_size) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "94b744648fb6eac5eb43185d25b489bd21aa564e"
   },
   "source": [
    "Let us now fit the model. This will start training the model and with the help of the checkpoints, you'll be able to save the model at the end of each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-24T09:33:39.785760Z",
     "start_time": "2018-12-24T09:33:24.008419Z"
    },
    "_uuid": "fc5f4eb739eb90d2eca8c5b1d5078652dd6c3b69"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "Source path =  ../input/data/Project_data/train ; batch size = 16\n",
      "Source path =  ../input/data/Project_data/val ; batch size = 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:21: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n",
      "/opt/conda/lib/python3.6/site-packages/skimage/transform/_warps.py:84: UserWarning: The default mode, 'constant', will be changed to 'reflect' in skimage 0.15.\n",
      "  warn(\"The default mode, 'constant', will be changed to 'reflect' in \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 7/42 [====>.........................] - ETA: 3:59 - loss: 1.7235 - categorical_accuracy: 0.2679"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:54: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42/42 [==============================] - 208s 5s/step - loss: 1.6383 - categorical_accuracy: 0.2657 - val_loss: 1.3852 - val_categorical_accuracy: 0.4400\n",
      "\n",
      "Epoch 00001: saving model to model_init_conv2d_lstm_2018-12-3003_00_58.183252/model-00001-1.64014-0.26546-1.38520-0.44000.h5\n",
      "Epoch 2/40\n",
      "42/42 [==============================] - 181s 4s/step - loss: 1.4253 - categorical_accuracy: 0.4027 - val_loss: 1.3144 - val_categorical_accuracy: 0.4500\n",
      "\n",
      "Epoch 00002: saving model to model_init_conv2d_lstm_2018-12-3003_00_58.183252/model-00002-1.42577-0.40422-1.31438-0.45000.h5\n",
      "Epoch 3/40\n",
      "42/42 [==============================] - 182s 4s/step - loss: 1.2899 - categorical_accuracy: 0.4634 - val_loss: 1.2529 - val_categorical_accuracy: 0.5000\n",
      "\n",
      "Epoch 00003: saving model to model_init_conv2d_lstm_2018-12-3003_00_58.183252/model-00003-1.29217-0.46003-1.25290-0.50000.h5\n",
      "Epoch 4/40\n",
      "42/42 [==============================] - 180s 4s/step - loss: 1.1972 - categorical_accuracy: 0.5322 - val_loss: 1.1361 - val_categorical_accuracy: 0.5600\n",
      "\n",
      "Epoch 00004: saving model to model_init_conv2d_lstm_2018-12-3003_00_58.183252/model-00004-1.19116-0.53544-1.13607-0.56000.h5\n",
      "Epoch 5/40\n",
      "42/42 [==============================] - 185s 4s/step - loss: 1.1482 - categorical_accuracy: 0.5657 - val_loss: 1.1389 - val_categorical_accuracy: 0.5800\n",
      "\n",
      "Epoch 00005: saving model to model_init_conv2d_lstm_2018-12-3003_00_58.183252/model-00005-1.14632-0.56561-1.13893-0.58000.h5\n",
      "Epoch 6/40\n",
      "42/42 [==============================] - 182s 4s/step - loss: 1.0935 - categorical_accuracy: 0.5817 - val_loss: 1.0650 - val_categorical_accuracy: 0.5400\n",
      "\n",
      "Epoch 00006: saving model to model_init_conv2d_lstm_2018-12-3003_00_58.183252/model-00006-1.09243-0.58371-1.06500-0.54000.h5\n",
      "Epoch 7/40\n",
      "42/42 [==============================] - 182s 4s/step - loss: 1.0434 - categorical_accuracy: 0.6282 - val_loss: 1.0134 - val_categorical_accuracy: 0.5500\n",
      "\n",
      "Epoch 00007: saving model to model_init_conv2d_lstm_2018-12-3003_00_58.183252/model-00007-1.04245-0.62896-1.01338-0.55000.h5\n",
      "Epoch 8/40\n",
      "42/42 [==============================] - 180s 4s/step - loss: 1.0011 - categorical_accuracy: 0.6446 - val_loss: 0.9827 - val_categorical_accuracy: 0.7200\n",
      "\n",
      "Epoch 00008: saving model to model_init_conv2d_lstm_2018-12-3003_00_58.183252/model-00008-0.99697-0.64555-0.98270-0.72000.h5\n",
      "Epoch 9/40\n",
      "42/42 [==============================] - 183s 4s/step - loss: 0.9403 - categorical_accuracy: 0.6807 - val_loss: 0.9503 - val_categorical_accuracy: 0.6300\n",
      "\n",
      "Epoch 00009: saving model to model_init_conv2d_lstm_2018-12-3003_00_58.183252/model-00009-0.93941-0.68024-0.95030-0.63000.h5\n",
      "Epoch 10/40\n",
      "42/42 [==============================] - 183s 4s/step - loss: 0.9202 - categorical_accuracy: 0.6628 - val_loss: 0.8807 - val_categorical_accuracy: 0.7100\n",
      "\n",
      "Epoch 00010: saving model to model_init_conv2d_lstm_2018-12-3003_00_58.183252/model-00010-0.91154-0.66968-0.88073-0.71000.h5\n",
      "Epoch 11/40\n",
      "42/42 [==============================] - 184s 4s/step - loss: 0.8898 - categorical_accuracy: 0.6866 - val_loss: 0.9001 - val_categorical_accuracy: 0.6800\n",
      "\n",
      "Epoch 00011: saving model to model_init_conv2d_lstm_2018-12-3003_00_58.183252/model-00011-0.88751-0.68627-0.90008-0.68000.h5\n",
      "Epoch 12/40\n",
      "42/42 [==============================] - 183s 4s/step - loss: 0.8537 - categorical_accuracy: 0.7082 - val_loss: 0.8965 - val_categorical_accuracy: 0.6600\n",
      "\n",
      "Epoch 00012: saving model to model_init_conv2d_lstm_2018-12-3003_00_58.183252/model-00012-0.84694-0.71192-0.89649-0.66000.h5\n",
      "Epoch 13/40\n",
      "42/42 [==============================] - 183s 4s/step - loss: 0.7919 - categorical_accuracy: 0.7287 - val_loss: 0.8679 - val_categorical_accuracy: 0.6600\n",
      "\n",
      "Epoch 00013: saving model to model_init_conv2d_lstm_2018-12-3003_00_58.183252/model-00013-0.78785-0.73454-0.86791-0.66000.h5\n",
      "Epoch 14/40\n",
      "42/42 [==============================] - 182s 4s/step - loss: 0.8056 - categorical_accuracy: 0.7153 - val_loss: 0.8074 - val_categorical_accuracy: 0.6900\n",
      "\n",
      "Epoch 00014: saving model to model_init_conv2d_lstm_2018-12-3003_00_58.183252/model-00014-0.79617-0.72097-0.80741-0.69000.h5\n",
      "Epoch 15/40\n",
      "42/42 [==============================] - 186s 4s/step - loss: 0.7351 - categorical_accuracy: 0.7547 - val_loss: 0.6970 - val_categorical_accuracy: 0.7400\n",
      "\n",
      "Epoch 00015: saving model to model_init_conv2d_lstm_2018-12-3003_00_58.183252/model-00015-0.72335-0.75716-0.69702-0.74000.h5\n",
      "Epoch 16/40\n",
      "42/42 [==============================] - 185s 4s/step - loss: 0.7034 - categorical_accuracy: 0.7562 - val_loss: 0.7122 - val_categorical_accuracy: 0.7500\n",
      "\n",
      "Epoch 00016: saving model to model_init_conv2d_lstm_2018-12-3003_00_58.183252/model-00016-0.69774-0.75867-0.71218-0.75000.h5\n",
      "Epoch 17/40\n",
      "42/42 [==============================] - 184s 4s/step - loss: 0.6455 - categorical_accuracy: 0.7871 - val_loss: 0.7106 - val_categorical_accuracy: 0.7300\n",
      "\n",
      "Epoch 00017: saving model to model_init_conv2d_lstm_2018-12-3003_00_58.183252/model-00017-0.63884-0.79186-0.71061-0.73000.h5\n",
      "Epoch 18/40\n",
      "42/42 [==============================] - 181s 4s/step - loss: 0.6607 - categorical_accuracy: 0.7797 - val_loss: 0.7209 - val_categorical_accuracy: 0.7100\n",
      "\n",
      "Epoch 00018: saving model to model_init_conv2d_lstm_2018-12-3003_00_58.183252/model-00018-0.65410-0.78431-0.72094-0.71000.h5\n",
      "Epoch 19/40\n",
      "42/42 [==============================] - 183s 4s/step - loss: 0.6028 - categorical_accuracy: 0.8210 - val_loss: 0.7042 - val_categorical_accuracy: 0.7100\n",
      "\n",
      "Epoch 00019: saving model to model_init_conv2d_lstm_2018-12-3003_00_58.183252/model-00019-0.60156-0.82051-0.70423-0.71000.h5\n",
      "Epoch 20/40\n",
      "42/42 [==============================] - 182s 4s/step - loss: 0.5822 - categorical_accuracy: 0.8102 - val_loss: 0.6978 - val_categorical_accuracy: 0.7400\n",
      "\n",
      "Epoch 00020: saving model to model_init_conv2d_lstm_2018-12-3003_00_58.183252/model-00020-0.58206-0.81146-0.69777-0.74000.h5\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 0.0019999999552965165.\n",
      "Epoch 21/40\n",
      "42/42 [==============================] - 182s 4s/step - loss: 0.5777 - categorical_accuracy: 0.8228 - val_loss: 0.6836 - val_categorical_accuracy: 0.7400\n",
      "\n",
      "Epoch 00021: saving model to model_init_conv2d_lstm_2018-12-3003_00_58.183252/model-00021-0.56365-0.82805-0.68358-0.74000.h5\n",
      "Epoch 22/40\n",
      "42/42 [==============================] - 187s 4s/step - loss: 0.5766 - categorical_accuracy: 0.8020 - val_loss: 0.6768 - val_categorical_accuracy: 0.7500\n",
      "\n",
      "Epoch 00022: saving model to model_init_conv2d_lstm_2018-12-3003_00_58.183252/model-00022-0.56730-0.80694-0.67677-0.75000.h5\n",
      "Epoch 23/40\n",
      "42/42 [==============================] - 183s 4s/step - loss: 0.5419 - categorical_accuracy: 0.8255 - val_loss: 0.6652 - val_categorical_accuracy: 0.7400\n",
      "\n",
      "Epoch 00023: saving model to model_init_conv2d_lstm_2018-12-3003_00_58.183252/model-00023-0.53729-0.82504-0.66518-0.74000.h5\n",
      "Epoch 24/40\n",
      "42/42 [==============================] - 184s 4s/step - loss: 0.5183 - categorical_accuracy: 0.8269 - val_loss: 0.6505 - val_categorical_accuracy: 0.7600\n",
      "\n",
      "Epoch 00024: saving model to model_init_conv2d_lstm_2018-12-3003_00_58.183252/model-00024-0.50928-0.83409-0.65054-0.76000.h5\n",
      "Epoch 25/40\n",
      "42/42 [==============================] - 182s 4s/step - loss: 0.5604 - categorical_accuracy: 0.8124 - val_loss: 0.6673 - val_categorical_accuracy: 0.7400\n",
      "\n",
      "Epoch 00025: saving model to model_init_conv2d_lstm_2018-12-3003_00_58.183252/model-00025-0.55023-0.81750-0.66727-0.74000.h5\n",
      "Epoch 26/40\n",
      "42/42 [==============================] - 184s 4s/step - loss: 0.5111 - categorical_accuracy: 0.8295 - val_loss: 0.6521 - val_categorical_accuracy: 0.7500\n",
      "\n",
      "Epoch 00026: saving model to model_init_conv2d_lstm_2018-12-3003_00_58.183252/model-00026-0.50554-0.83107-0.65215-0.75000.h5\n",
      "Epoch 27/40\n",
      "42/42 [==============================] - 182s 4s/step - loss: 0.5194 - categorical_accuracy: 0.8336 - val_loss: 0.6280 - val_categorical_accuracy: 0.7700\n",
      "\n",
      "Epoch 00027: saving model to model_init_conv2d_lstm_2018-12-3003_00_58.183252/model-00027-0.51337-0.83710-0.62796-0.77000.h5\n",
      "Epoch 28/40\n",
      "42/42 [==============================] - 183s 4s/step - loss: 0.5247 - categorical_accuracy: 0.8269 - val_loss: 0.6149 - val_categorical_accuracy: 0.7500\n",
      "\n",
      "Epoch 00028: saving model to model_init_conv2d_lstm_2018-12-3003_00_58.183252/model-00028-0.51455-0.83409-0.61486-0.75000.h5\n",
      "Epoch 29/40\n",
      "42/42 [==============================] - 183s 4s/step - loss: 0.4768 - categorical_accuracy: 0.8582 - val_loss: 0.6312 - val_categorical_accuracy: 0.7200\n",
      "\n",
      "Epoch 00029: saving model to model_init_conv2d_lstm_2018-12-3003_00_58.183252/model-00029-0.47188-0.85822-0.63117-0.72000.h5\n",
      "Epoch 30/40\n",
      "42/42 [==============================] - 184s 4s/step - loss: 0.5064 - categorical_accuracy: 0.8366 - val_loss: 0.6101 - val_categorical_accuracy: 0.7700\n",
      "\n",
      "Epoch 00030: saving model to model_init_conv2d_lstm_2018-12-3003_00_58.183252/model-00030-0.49681-0.84012-0.61014-0.77000.h5\n",
      "Epoch 31/40\n",
      "42/42 [==============================] - 182s 4s/step - loss: 0.4890 - categorical_accuracy: 0.8407 - val_loss: 0.6318 - val_categorical_accuracy: 0.7700\n",
      "\n",
      "Epoch 00031: saving model to model_init_conv2d_lstm_2018-12-3003_00_58.183252/model-00031-0.48031-0.84615-0.63180-0.77000.h5\n",
      "Epoch 32/40\n",
      "42/42 [==============================] - 182s 4s/step - loss: 0.4881 - categorical_accuracy: 0.8396 - val_loss: 0.5877 - val_categorical_accuracy: 0.7600\n",
      "\n",
      "Epoch 00032: saving model to model_init_conv2d_lstm_2018-12-3003_00_58.183252/model-00032-0.48088-0.84314-0.58775-0.76000.h5\n",
      "Epoch 33/40\n",
      "42/42 [==============================] - 183s 4s/step - loss: 0.4641 - categorical_accuracy: 0.8440 - val_loss: 0.5891 - val_categorical_accuracy: 0.7800\n",
      "\n",
      "Epoch 00033: saving model to model_init_conv2d_lstm_2018-12-3003_00_58.183252/model-00033-0.45967-0.84766-0.58913-0.78000.h5\n",
      "Epoch 34/40\n",
      "42/42 [==============================] - 182s 4s/step - loss: 0.4653 - categorical_accuracy: 0.8377 - val_loss: 0.5750 - val_categorical_accuracy: 0.7800\n",
      "\n",
      "Epoch 00034: saving model to model_init_conv2d_lstm_2018-12-3003_00_58.183252/model-00034-0.45658-0.84314-0.57497-0.78000.h5\n",
      "Epoch 35/40\n",
      "42/42 [==============================] - 182s 4s/step - loss: 0.4639 - categorical_accuracy: 0.8515 - val_loss: 0.5832 - val_categorical_accuracy: 0.7500\n",
      "\n",
      "Epoch 00035: saving model to model_init_conv2d_lstm_2018-12-3003_00_58.183252/model-00035-0.45278-0.85520-0.58323-0.75000.h5\n",
      "Epoch 36/40\n",
      "42/42 [==============================] - 182s 4s/step - loss: 0.4704 - categorical_accuracy: 0.8604 - val_loss: 0.5808 - val_categorical_accuracy: 0.7800\n",
      "\n",
      "Epoch 00036: saving model to model_init_conv2d_lstm_2018-12-3003_00_58.183252/model-00036-0.45745-0.86425-0.58076-0.78000.h5\n",
      "Epoch 37/40\n",
      "42/42 [==============================] - 184s 4s/step - loss: 0.4448 - categorical_accuracy: 0.8638 - val_loss: 0.5824 - val_categorical_accuracy: 0.7700\n",
      "\n",
      "Epoch 00037: saving model to model_init_conv2d_lstm_2018-12-3003_00_58.183252/model-00037-0.44313-0.86576-0.58243-0.77000.h5\n",
      "Epoch 38/40\n",
      "42/42 [==============================] - 181s 4s/step - loss: 0.4512 - categorical_accuracy: 0.8600 - val_loss: 0.5818 - val_categorical_accuracy: 0.7300\n",
      "\n",
      "Epoch 00038: saving model to model_init_conv2d_lstm_2018-12-3003_00_58.183252/model-00038-0.43876-0.86576-0.58177-0.73000.h5\n",
      "Epoch 39/40\n",
      "42/42 [==============================] - 184s 4s/step - loss: 0.4464 - categorical_accuracy: 0.8556 - val_loss: 0.5736 - val_categorical_accuracy: 0.7600\n",
      "\n",
      "Epoch 00039: saving model to model_init_conv2d_lstm_2018-12-3003_00_58.183252/model-00039-0.43870-0.86124-0.57364-0.76000.h5\n",
      "Epoch 40/40\n",
      "42/42 [==============================] - 181s 4s/step - loss: 0.4355 - categorical_accuracy: 0.8612 - val_loss: 0.5786 - val_categorical_accuracy: 0.7500\n",
      "\n",
      "Epoch 00040: saving model to model_init_conv2d_lstm_2018-12-3003_00_58.183252/model-00040-0.42386-0.86878-0.57864-0.75000.h5\n"
     ]
    }
   ],
   "source": [
    "conv2d_lstm_model = model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "                              callbacks=callbacks_list, validation_data=val_generator, \n",
    "                              validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "cd70bf55a005aec1bda41445e22aa9699ecb1467"
   },
   "source": [
    "#### Building graphs #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_uuid": "2ae64df4b4692b03117c680e4a5bfcddc60bac9a"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(conv2d_lstm_model.history[\"categorical_accuracy\"])\n",
    "plt.plot(conv2d_lstm_model.history[\"val_categorical_accuracy\"])\n",
    "plt.xlabel(\"EPOCHS\")\n",
    "plt.ylabel(\"Percentage\")\n",
    "plt.title(\"Conv2D+LSTM Model\")\n",
    "plt.legend([\"Train\",\"Test\"],loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_uuid": "c61d4cafd53135c93eeb6f93dbb0478dd837c497"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(conv2d_lstm_model.history[\"loss\"])\n",
    "plt.plot(conv2d_lstm_model.history[\"val_loss\"])\n",
    "plt.xlabel(\"EPOCHS\")\n",
    "plt.ylabel(\"LOSS\")\n",
    "plt.ylim(0.0,3.0)\n",
    "plt.title(\"Conv2D+LSTM Model (LOSS)\")\n",
    "plt.legend([\"Train\",\"Testing\"],loc=\"upper right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "41c6075e3c4481570477aa1b4df1179cc44b3cc1"
   },
   "source": [
    "#### Checking the EPOCH index where maximum validation accuracy was achieved ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_uuid": "dfc3947f6d48065e9f19fbc58362c577ec3472cb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum Validation Accuracy = 0.780000 achieved at epoch iteration = 33\n"
     ]
    }
   ],
   "source": [
    "print(\"Maximum Validation Accuracy = %f achieved at epoch iteration = %d\" %(np.array(conv2d_lstm_model.history[\"val_categorical_accuracy\"]).max(),np.array(conv2d_lstm_model.history[\"val_categorical_accuracy\"]).argmax()+1))\n",
    "sel_model_idx = np.array(conv2d_lstm_model.history[\"val_categorical_accuracy\"]).argmax() + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "06ac8269b87deb1dc5f63051c9982140858b38a4"
   },
   "source": [
    "#### Selecting h5 file for loading model ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "_uuid": "8640781c7c0c1337b89f6f629627b7585b9e05c4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected model name : model_init_conv2d_lstm_2018-12-3003_00_58.183252/model-00033-0.45967-0.84766-0.58913-0.78000.h5\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "search_string = \"model-0*\" + str(sel_model_idx) + \"-[a-z0-9\\-\\.]*\"\n",
    "selected_model_name = model_name + list(filter(lambda x: re.search(search_string,x),os.listdir(model_name)))[0]\n",
    "print(\"Selected model name : %s\" %(selected_model_name))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "0b2b2dcaa90e7b832974981b3473c6059430dd20"
   },
   "source": [
    "#### Loading model weights from h5 file ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "_uuid": "3aea2c1fe28d66a5f0a509e760455ab81baa6322"
   },
   "outputs": [],
   "source": [
    "from keras.models import clone_model\n",
    "selected_model = clone_model(model)\n",
    "selected_model.load_weights(selected_model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b36c824aea945e585d8ef838b64e9810ff95699e"
   },
   "source": [
    "#### Performing prediction on validation data ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "_uuid": "6c4245eb166db86e24cb4ae1fb6fa5ebcd1b384a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source path =  ../input/data/Project_data/val ; batch size = 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:21: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n",
      "/opt/conda/lib/python3.6/site-packages/skimage/transform/_warps.py:84: UserWarning: The default mode, 'constant', will be changed to 'reflect' in skimage 0.15.\n",
      "  warn(\"The default mode, 'constant', will be changed to 'reflect' in \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 0s 11ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "16/16 [==============================] - 0s 3ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:54: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 8ms/step\n"
     ]
    }
   ],
   "source": [
    "testing_val_generator = generator(val_path,\n",
    "                          val_data, \n",
    "                          batch_size=batch_size,\n",
    "                          transform_size=transform_size,\n",
    "                          frame_selection_list=frame_selection_list,\n",
    "                          process_input_func=preprocess_input_func,\n",
    "                          base_model=base_model)\n",
    "result = None\n",
    "for x in range(validation_steps):\n",
    "    x_val,y_val = next(testing_val_generator)\n",
    "    p_val = selected_model.predict(x_val,batch_size=batch_size,verbose=1)\n",
    "    if x == 0:\n",
    "        result = np.vstack((y_val.argmax(axis=1),p_val.argmax(axis=1)))\n",
    "    else:\n",
    "        result = np.hstack((result,np.vstack((y_val.argmax(axis=1),p_val.argmax(axis=1)))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e4ca752a58f572224618e8dc9fdac4a1e4ba068b"
   },
   "source": [
    "#### Consolidating Results ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "_uuid": "dec1b8b1a4535c8083e4f8011f576d74ddc511c5"
   },
   "outputs": [],
   "source": [
    "result = pd.DataFrame(np.transpose(result),columns=[\"Actual\",\"Predicted\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "_uuid": "cb8e609382948087a55dafb6b89f1ffd1a007f9f"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix,classification_report,accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-30T13:59:47.528573Z",
     "start_time": "2018-12-30T13:59:47.507558Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Predicted_Left</th>\n",
       "      <th>Predicted_Right</th>\n",
       "      <th>Predicted_Stop</th>\n",
       "      <th>Predicted_Down</th>\n",
       "      <th>Predicted_Up</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Actual_Left</th>\n",
       "      <td>11</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual_Right</th>\n",
       "      <td>5</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual_Stop</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual_Down</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual_Up</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Predicted_Left  Predicted_Right  Predicted_Stop  Predicted_Down  \\\n",
       "Actual_Left               11                5               1               0   \n",
       "Actual_Right               5               17               1               0   \n",
       "Actual_Stop                0                0              19               2   \n",
       "Actual_Down                0                0               0              20   \n",
       "Actual_Up                  0                0               4               0   \n",
       "\n",
       "              Predicted_Up  \n",
       "Actual_Left              1  \n",
       "Actual_Right             0  \n",
       "Actual_Stop              1  \n",
       "Actual_Down              1  \n",
       "Actual_Up               12  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cf_np = confusion_matrix(result.Actual,result.Predicted,labels=range(0,5))\n",
    "cf_df = pd.DataFrame(cf_np,columns=[\"Predicted_\" + str(x) for x in [\"Left\",\"Right\",\"Stop\",\"Down\",\"Up\"]],index=[\"Actual_\" + str(x) for x in [\"Left\",\"Right\",\"Stop\",\"Down\",\"Up\"]])\n",
    "cf_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "_uuid": "641a6e8a3fb2c7ee2fc6db82d31473f809b160fd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cummalative Accuracy Score : 0.790000\n"
     ]
    }
   ],
   "source": [
    "print(\"Cummalative Accuracy Score : %f\" %(accuracy_score(result.Actual,result.Predicted)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
